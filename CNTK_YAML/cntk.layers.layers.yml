api_name: []
items:
- _type: module
  children: []
  module: cntk.layers.layers
  name: cntk.layers.layers
  summary: ''
  type: Namespace
  uid: cntk.layers.layers
- _type: class
  children:
  - cntk.layers.layers.Activation
  - cntk.layers.layers.AveragePooling
  - cntk.layers.layers.BatchNormalization
  - cntk.layers.layers.Convolution
  - cntk.layers.layers.Convolution1D
  - cntk.layers.layers.Convolution2D
  - cntk.layers.layers.Convolution3D
  - cntk.layers.layers.ConvolutionTranspose
  - cntk.layers.layers.ConvolutionTranspose1D
  - cntk.layers.layers.ConvolutionTranspose2D
  - cntk.layers.layers.ConvolutionTranspose3D
  - cntk.layers.layers.Dense
  - cntk.layers.layers.Dropout
  - cntk.layers.layers.Embedding
  - cntk.layers.layers.GlobalAveragePooling
  - cntk.layers.layers.GlobalMaxPooling
  - cntk.layers.layers.Label
  - cntk.layers.layers.LayerNormalization
  - cntk.layers.layers.MaxPooling
  - cntk.layers.layers.MaxUnpooling
  module: cntk.layers.layers
  name: cntk.layers.layers.Global
  summary: Proxy object to hold module level functions
  type: Class
  uid: cntk.layers.layers.Global
- _type: function
  module: cntk.layers.layers
  name: cntk.layers.layers.Activation
  summary: "Layer factory function to create an activation layer.\nActivation functions\
    \ can be used directly in CNTK, so there is no difference between\n``y = relu(x)``\
    \ and ``y = Activation(relu)(x)``.\nThis layer is useful if one wants to configure\
    \ the activation function\nwith ``default_options``, or when its invocation should\
    \ be named.\n\nExample:\n >>> model = Dense(500) >> Activation(C.relu) >> Dense(10)\n\
    \ >>> # is the same as\n >>> model = Dense(500) >> C.relu >> Dense(10)\n >>> #\
    \ and also the same as\n >>> model = Dense(500, activation=C.relu) >> Dense(10)\n\
    \nArgs:\n activation (:class:`~cntk.ops.functions.Function`, defaults to `identity`):\
    \ function to apply at the end, e.g. `relu`\n name (str, defaults to ''): the\
    \ name of the function instance in the network\n\nReturns:\n    cntk.ops.functions.Function:\n\
    \    A function that accepts one argument and applies the operation to it\n"
  type: Method
  uid: cntk.layers.layers.Activation
- _type: function
  module: cntk.layers.layers
  name: cntk.layers.layers.AveragePooling
  summary: "Layer factory function to create an average-pooling layer.\n\nLike ``Convolution()``,\
    \ ``AveragePooling()`` processes items arranged on an N-dimensional grid, such\
    \ as an image.\nTypically, each item is a vector.\nFor each item, average-pooling\
    \ computes the element-wise mean over a window (\"receptive field\") of items\
    \ surrounding the item's position on the grid.\n\nThe size (spatial extent) of\
    \ the receptive field is given by ``filter_shape``.\nE.g. for 2D pooling, ``filter_shape``\
    \ should be a tuple of two integers, such as `(5,5)`.\n\nExample:\n >>> f = AveragePooling((3,3),\
    \ strides=2)  # reduce dimensionality by 2, pooling over windows of 3x3\n >>>\
    \ h = input((32,240,320))  # e.g. 32-dim feature map\n >>> hp = f(h)\n >>> hp.shape\
    \  # spatial dimension has been halved due to stride, and lost one due to 3x3\
    \ window without padding\n     (32, 119, 159)\n\n >>> f = AveragePooling((2,2),\
    \ strides=2)\n >>> f.update_signature((1,4,4))\n >>> im = np.array([[[3, 5, 2,\
    \ 6], [4, 2, 8, 3], [1, 6, 4, 7], [7, 3, 5, 9]]])  # a 4x4 image (feature-map\
    \ depth 1 for simplicity)\n >>> im\n     array([[[3, 5, 2, 6],\n             [4,\
    \ 2, 8, 3],\n             [1, 6, 4, 7],\n             [7, 3, 5, 9]]])\n >>> f([[im]])\
    \  # due to strides=2, this computes the averages of each 2x2 sub-block\n    \
    \ array([[[[ 3.5 ,  4.75],\n              [ 4.25,  6.25]]]], dtype=float32)\n\n\
    Args:\n filter_shape (`int` or `tuple` of `ints`): shape (spatial extent) of the\
    \ receptive field, *not* including the input feature-map depth. E.g. (3,3) for\
    \ a 2D convolution.\n strides (`int` or `tuple` of `ints`, defaults to 1): stride\
    \ (increment when sliding over the input). Use a `tuple` to specify a per-axis\
    \ value.\n pad (`bool` or `tuple` of `bools`, defaults to `False`): if `False`,\
    \ then the pooling operation will be shifted over the \"valid\"\n  area of input,\
    \ that is, no value outside the area is used. If ``pad=True`` on the other hand,\n\
    \  pooling will be applied to all input positions, and positions outside the valid\
    \ region will be excluded from the averaging.\n  Use a `tuple` to specify a per-axis\
    \ value.\n name (str, defaults to ''): the name of the function instance in the\
    \ network\n\nReturns:\n    cntk.ops.functions.Function:\n    A function that accepts\
    \ one argument and applies the average-pooling operation to it\n"
  type: Method
  uid: cntk.layers.layers.AveragePooling
- _type: function
  module: cntk.layers.layers
  name: cntk.layers.layers.BatchNormalization
  summary: "Layer factory function to create a batch-normalization layer.\n\nBatch\
    \ normalization applies this formula to every input element (element-wise):\n\
    ``y = (x - batch_mean) / (batch_stddev + epsilon) * scale + bias``\nwhere ``batch_mean``\
    \ and ``batch_stddev`` are estimated on the minibatch and ``scale`` and ``bias``\
    \ are learned parameters.\nTODO: add paper reference\n\nDuring operation, this\
    \ layer also estimates an aggregate running mean and standard deviation for use\
    \ in inference.\n\nA ``BatchNormalization`` layer instance owns its learnable\
    \ parameter tensors and exposes them as attributes ``.scale`` and ``.bias``.\n\
    The aggregate estimates are exposed as attributes ``aggregate_mean``, ``aggregate_variance``,\
    \ and ``aggregate_count``.\n\nExample:\n >>> # BatchNorm on an image with spatial\
    \ pooling\n >>> f = BatchNormalization(map_rank=1)\n >>> f.update_signature((3,480,640))\n\
    \ >>> f.bias.shape, f.scale.shape  # due to spatial pooling (map_rank=1), there\
    \ are only 3 biases and scales, shared across all pixel positions\n     ((3,),\
    \ (3,))\n\nArgs:\n map_rank (1 or ``None``): passing 1 means spatially-pooled\
    \ batch-normalization, where normalization values will be tied across all pixel\
    \ positions; while ``None``\n  will normalize all elements of the input tensor\
    \ independently\n init_scale (float, default 1): initial value for the ``scale``\
    \ parameter\n normalization_time_constant (int, default 5000): time constant for\
    \ smoothing the batch statistics in order to compute aggregate estimates for inference.\n\
    \ epsilon (float, default 0.00001): epsilon added to the variance to avoid division\
    \ by 0\n use_cntk_engine (bool, default ``False``): if ``True`` then use CNTK's\
    \ own engine instead of NVidia's.\n name (str, optional): the name of the function\
    \ instance in the network\n\nReturns:\n    cntk.ops.functions.Function:\n    A\
    \ function that accepts one argument and applies the operation to it\n"
  type: Method
  uid: cntk.layers.layers.BatchNormalization
- _type: function
  module: cntk.layers.layers
  name: cntk.layers.layers.Convolution
  summary: "Layer factory function to create a convolution layer.\n\nThis implements\
    \ a convolution operation over items arranged on an N-dimensional grid, such as\
    \ pixels in an image.\nTypically, each item is a vector (e.g. pixel: R,G,B), and\
    \ the result is, in turn, a vector.\nThe item-grid dimensions are referred to\
    \ as the *spatial* dimensions (e.g. dimensions of an image),\nwhile the vector\
    \ dimension of the individual items is often called *feature-map depth*.\n\nFor\
    \ each item, convolution gathers a window (\"receptive field\") of items surrounding\
    \ the item's position on the grid,\nand applies a little fully-connected network\
    \ to it (the same little network is applied to all item positions).\nThe size\
    \ (spatial extent) of the receptive field is given by ``filter_shape``.\nE.g.\
    \ to specify a 2D convolution, ``filter_shape`` should be a tuple of two integers,\
    \ such as `(5,5)`;\nan example for a 3D convolution (e.g. video or an MRI scan)\
    \ would be ``filter_shape=(3,3,3)``;\nwhile for a 1D convolution (e.g. audio or\
    \ text), ``filter_shape`` has one element, such as (3,) or just 3.\n\nThe dimension\
    \ of the input items (input feature-map depth) is not to be specified. It is known\
    \ from the input.\nThe dimension of the output items (output feature-map depth)\
    \ generated for each item position is given by ``num_filters``.\n\nIf the input\
    \ is a sequence, the sequence elements are by default treated independently.\n\
    To convolve along the sequence dimension as well, pass ``sequential=True``.\n\
    This is useful for variable-length inputs, such as video\nor natural-language\
    \ processing (word n-grams).\nNote, however, that convolution does not support\
    \ sparse inputs.\n\nBoth input and output items can be scalars intead of vectors.\
    \ For scalar-valued input items,\nsuch as pixels on a black-and-white image, or\
    \ samples of an audio clip, specify ``reduction_rank=0``.\nIf the output items\
    \ are scalar, pass ``num_filters=()`` or ``None``.\n\nA ``Convolution`` instance\
    \ owns its weight parameter tensors `W` and `b`, and exposes them as an attributes\
    \ ``.W`` and ``.b``.\nThe weights will have the shape ``(num_filters, input_feature_map_depth,\
    \ *filter_shape)``\n\nExample:\n >>> # 2D convolution of 5x4 receptive field with\
    \ output feature-map depth 128:\n >>> f = Convolution((5,4), 128, activation=C.relu)\n\
    \ >>> x = input((3,480,640))  # 3-channel color image\n >>> h = f(x)\n >>> h.shape\n\
    \     (128, 476, 637)\n >>> f.W.shape  # will have the form (num_filters, input_depth,\
    \ *filter_shape)\n     (128, 3, 5, 4)\n\n >>> # 2D convolution over a one-channel\
    \ black-and-white image, padding, and stride 2 along width dimension\n >>> f =\
    \ Convolution((3,3), 128, reduction_rank=0, pad=True, strides=(1,2), activation=C.relu)\n\
    \ >>> x = input((480,640))\n >>> h = f(x)\n >>> h.shape\n     (128, 480, 320)\n\
    \ >>> f.W.shape\n     (128, 1, 3, 3)\n\n >>> # 3D convolution along dynamic axis\
    \ over a sequence of 2D color images\n >>> from cntk.layers.typing import Sequence,\
    \ Tensor\n >>> f = Convolution((2,5,4), 128, sequential=True, activation=C.relu)\
    \ # over 2 consecutive frames\n >>> x = input(**Sequence[Tensor[3,480,640]]) \
    \ # a variable-length video of 640x480 RGB images\n >>> h = f(x)\n >>> h.shape\
    \   # this is the shape per video frame: 637x476 activation vectors of length\
    \ 128 each\n     (128, 476, 637)\n >>> f.W.shape # (output featuer map depth,\
    \ input depth, and the three filter dimensions)\n     (128, 3, 2, 5, 4)\n\nArgs:\n\
    \ filter_shape (`int` or `tuple` of `ints`): shape (spatial extent) of the receptive\
    \ field, *not* including the input feature-map depth. E.g. (3,3) for a 2D convolution.\n\
    \ num_filters (int, defaults to `None`): number of filters (output feature-map\
    \ depth), or ``()`` to denote scalar output items (output shape will have no depth\
    \ axis).\n sequential (bool, defaults to `False`): if `True`, also convolve along\
    \ the dynamic axis. ``filter_shape[0]`` corresponds to dynamic axis.\n activation\
    \ (:class:`~cntk.ops.functions.Function`, defaults to `identity`): optional function\
    \ to apply at the end, e.g. `relu`\n init (scalar or NumPy array or :mod:`cntk.initializer`,\
    \ defaults to :func:`~cntk.initializer.glorot_uniform` ): initial value of weights\
    \ `W`\n pad (`bool` or `tuple` of `bools`, defaults to `False`): if `False`, then\
    \ the filter will be shifted over the \"valid\"\n  area of input, that is, no\
    \ value outside the area is used. If ``pad=True`` on the other hand,\n  the filter\
    \ will be applied to all input positions, and positions outside the valid region\
    \ will be considered containing zero.\n  Use a `tuple` to specify a per-axis value.\n\
    \ strides (`int` or `tuple` of `ints`, defaults to 1): stride of the convolution\
    \ (increment when sliding the filter over the input). Use a `tuple` to specify\
    \ a per-axis value.\n sharing (bool, defaults to `True`): When `True`, every position\
    \ uses the same Convolution kernel.  When `False`, you can have a different Convolution\
    \ kernel per position, but `False` is not supported.\n bias (bool, optional, defaults\
    \ to `True`): the layer will have no bias if `False` is passed here\n init_bias\
    \ (scalar or NumPy array or :mod:`cntk.initializer`, defaults to 0): initial value\
    \ of weights `b`\n reduction_rank (`int`, defaults to 1): set to 0 if input items\
    \ are scalars (input has no depth axis), e.g. an audio signal or a black-and-white\
    \ image\n  that is stored with tensor shape (H,W) instead of (1,H,W)\n transpose_weight\
    \ (bool, defaults to `False`): When this is `True` this is convolution, otherwise\
    \ this is correlation (which is common for most toolkits)\n max_temp_mem_size_in_samples\
    \ (int, defaults to 0): Limits the amount of memory for intermiadate convolution\
    \ results.  A value of 0 means, memory is automatically managed.\n name (str,\
    \ defaults to ''): the name of the function instance in the network\n\nReturns:\n\
    \    cntk.ops.functions.Function:\n    A function that accepts one argument and\
    \ applies the convolution operation to it\n"
  type: Method
  uid: cntk.layers.layers.Convolution
- _type: function
  module: cntk.layers.layers
  name: cntk.layers.layers.Convolution1D
  summary: "Layer factory function to create a 1D convolution layer with optional\
    \ non-linearity.\nSame as `Convolution()` except that filter_shape is verified\
    \ to be 1-dimensional.\nSee `Convolution()` for extensive documentation.\n\nArgs:\n\
    \ filter_shape (`int` or `tuple` of `ints`): shape (spatial extent) of the receptive\
    \ field, *not* including the input feature-map depth. E.g. (3,3) for a 2D convolution.\n\
    \ num_filters (int, defaults to `None`): number of filters (output feature-map\
    \ depth), or ``()`` to denote scalar output items (output shape will have no depth\
    \ axis).\n activation (:class:`~cntk.ops.functions.Function`, defaults to `identity`):\
    \ optional function to apply at the end, e.g. `relu`\n init (scalar or NumPy array\
    \ or :mod:`cntk.initializer`, defaults to :func:`~cntk.initializer.glorot_uniform`\
    \ ): initial value of weights `W`\n pad (`bool` or `tuple` of `bools`, defaults\
    \ to `False`): if `False`, then the filter will be shifted over the \"valid\"\n\
    \  area of input, that is, no value outside the area is used. If ``pad=True``\
    \ on the other hand,\n  the filter will be applied to all input positions, and\
    \ positions outside the valid region will be considered containing zero.\n  Use\
    \ a `tuple` to specify a per-axis value.\n strides (`int` or `tuple` of `ints`,\
    \ defaults to 1): stride of the convolution (increment when sliding the filter\
    \ over the input). Use a `tuple` to specify a per-axis value.\n bias (bool, defaults\
    \ to `True`): the layer will have no bias if `False` is passed here\n init_bias\
    \ (scalar or NumPy array or :mod:`cntk.initializer`, defaults to 0): initial value\
    \ of weights `b`\n reduction_rank (`int`, defaults to 1): set to 0 if input items\
    \ are scalars (input has no depth axis), e.g. an audio signal or a black-and-white\
    \ image\n  that is stored with tensor shape (H,W) instead of (1,H,W)\n name (str,\
    \ defaults to ''): the name of the function instance in the network\n\nReturns:\n\
    \    cntk.ops.functions.Function:\n    A function that accepts one argument and\
    \ applies the convolution operation to it\n"
  type: Method
  uid: cntk.layers.layers.Convolution1D
- _type: function
  module: cntk.layers.layers
  name: cntk.layers.layers.Convolution2D
  summary: "Layer factory function to create a 2D convolution layer with optional\
    \ non-linearity.\nSame as `Convolution()` except that filter_shape is verified\
    \ to be 2-dimensional.\nSee `Convolution()` for extensive documentation.\n\nArgs:\n\
    \ filter_shape (`int` or `tuple` of `ints`): shape (spatial extent) of the receptive\
    \ field, *not* including the input feature-map depth. E.g. (3,3) for a 2D convolution.\n\
    \ num_filters (int, defaults to `None`): number of filters (output feature-map\
    \ depth), or ``()`` to denote scalar output items (output shape will have no depth\
    \ axis).\n activation (:class:`~cntk.ops.functions.Function`, defaults to `identity`):\
    \ optional function to apply at the end, e.g. `relu`\n init (scalar or NumPy array\
    \ or :mod:`cntk.initializer`, defaults to :func:`~cntk.initializer.glorot_uniform`\
    \ ): initial value of weights `W`\n pad (`bool` or `tuple` of `bools`, defaults\
    \ to `False`): if `False`, then the filter will be shifted over the \"valid\"\n\
    \  area of input, that is, no value outside the area is used. If ``pad=True``\
    \ on the other hand,\n  the filter will be applied to all input positions, and\
    \ positions outside the valid region will be considered containing zero.\n  Use\
    \ a `tuple` to specify a per-axis value.\n strides (`int` or `tuple` of `ints`,\
    \ defaults to 1): stride of the convolution (increment when sliding the filter\
    \ over the input). Use a `tuple` to specify a per-axis value.\n bias (bool, defaults\
    \ to `True`): the layer will have no bias if `False` is passed here\n init_bias\
    \ (scalar or NumPy array or :mod:`cntk.initializer`, defaults to 0): initial value\
    \ of weights `b`\n reduction_rank (`int`, defaults to 1): set to 0 if input items\
    \ are scalars (input has no depth axis), e.g. an audio signal or a black-and-white\
    \ image\n  that is stored with tensor shape (H,W) instead of (1,H,W)\n name (str,\
    \ defaults to ''): the name of the function instance in the network\n\nReturns:\n\
    \    cntk.ops.functions.Function:\n    A function that accepts one argument and\
    \ applies the convolution operation to it\n"
  type: Method
  uid: cntk.layers.layers.Convolution2D
- _type: function
  module: cntk.layers.layers
  name: cntk.layers.layers.Convolution3D
  summary: "Layer factory function to create a 3D convolution layer with optional\
    \ non-linearity.\nSame as `Convolution()` except that filter_shape is verified\
    \ to be 3-dimensional.\nSee `Convolution()` for extensive documentation.\n\nArgs:\n\
    \ filter_shape (`int` or `tuple` of `ints`): shape (spatial extent) of the receptive\
    \ field, *not* including the input feature-map depth. E.g. (3,3) for a 2D convolution.\n\
    \ num_filters (int, defaults to `None`): number of filters (output feature-map\
    \ depth), or ``()`` to denote scalar output items (output shape will have no depth\
    \ axis).\n activation (:class:`~cntk.ops.functions.Function`, defaults to `identity`):\
    \ optional function to apply at the end, e.g. `relu`\n init (scalar or NumPy array\
    \ or :mod:`cntk.initializer`, defaults to :func:`~cntk.initializer.glorot_uniform`\
    \ ): initial value of weights `W`\n pad (`bool` or `tuple` of `bools`, defaults\
    \ to `False`): if `False`, then the filter will be shifted over the \"valid\"\n\
    \  area of input, that is, no value outside the area is used. If ``pad=True``\
    \ on the other hand,\n  the filter will be applied to all input positions, and\
    \ positions outside the valid region will be considered containing zero.\n  Use\
    \ a `tuple` to specify a per-axis value.\n strides (`int` or `tuple` of `ints`,\
    \ defaults to 1): stride of the convolution (increment when sliding the filter\
    \ over the input). Use a `tuple` to specify a per-axis value.\n bias (bool, defaults\
    \ to `True`): the layer will have no bias if `False` is passed here\n init_bias\
    \ (scalar or NumPy array or :mod:`cntk.initializer`, defaults to 0): initial value\
    \ of weights `b`\n reduction_rank (`int`, defaults to 1): set to 0 if input items\
    \ are scalars (input has no depth axis), e.g. an audio signal or a black-and-white\
    \ image\n  that is stored with tensor shape (H,W) instead of (1,H,W)\n name (str,\
    \ defaults to ''): the name of the function instance in the network\n\nReturns:\n\
    \    cntk.ops.functions.Function:\n    A function that accepts one argument and\
    \ applies the convolution operation to it\n"
  type: Method
  uid: cntk.layers.layers.Convolution3D
- _type: function
  module: cntk.layers.layers
  name: cntk.layers.layers.ConvolutionTranspose
  summary: "Layer factory function to create a convolution transpose layer.\n\nThis\
    \ implements a convolution_transpose operation over items arranged on an N-dimensional\
    \ grid, such as pixels in an image.\nTypically, each item is a vector (e.g. pixel:\
    \ R,G,B), and the result is, in turn, a vector.\nThe item-grid dimensions are\
    \ referred to as the *spatial* dimensions (e.g. dimensions of an image),\nwhile\
    \ the vector dimensions of the individual items are often called *feature-map\
    \ depth*.\n\nConvolution transpose is also known as ``fractionally strided convolutional\
    \ layers``, or, ``deconvolution``. \nThis operation is used in image and language\
    \ processing applications. It supports arbitrary\ndimensions, strides, and padding.\
    \ \n\nThe forward and backward computation of convolution transpose is the inverse\
    \ of convolution. That is, during forward\npass the input layer's items are spread\
    \ into the output same as the backward spread of gradients in convolution. The\
    \ \nbackward pass, on the other hand, performs a convolution same as the forward\
    \ pass of convolution. \n\nThe size (spatial extent) of the receptive field for\
    \ convolution transpose is given by ``filter_shape``.\nE.g. to specify a 2D convolution\
    \ transpose, ``filter_shape`` should be a tuple of two integers, such as `(5,5)`;\n\
    an example for a 3D convolution transpose (e.g. video or an MRI scan) would be\
    \ ``filter_shape=(3,3,3)``;\nwhile for a 1D convolution transpose (e.g. audio\
    \ or text), ``filter_shape`` has one element, such as (3,).\n\nThe dimension of\
    \ the input items (feature-map depth) is not specified, but known from the input.\n\
    The dimension of the output items generated for each item position is given by\
    \ ``num_filters``.\n\nA ``ConvolutionTranspose`` instance owns its weight parameter\
    \ tensors `W` and `b`, and exposes them as an attributes ``.W`` and ``.b``.\n\
    The weights will have the shape ``(input_feature_map_depth, num_filters, *filter_shape)``.\
    \ \n\nExample:\n >>> # 2D convolution transpose of 3x4 receptive field with output\
    \ feature-map depth 128:\n >>> f = ConvolutionTranspose((3,4), 128, activation=C.relu)\n\
    \ >>> x = input((3,480,640))  # 3-channel color image\n >>> h = f(x)\n >>> h.shape\n\
    \     (128, 482, 643)\n >>> f.W.shape  # will have the form (input_depth, num_filters,\
    \ *filter_shape)\n     (3, 128, 3, 4)\n\nArgs:\n filter_shape (`int` or tuple\
    \ of `int`\\ s): shape (spatial extent) of the receptive field, *not* including\
    \ the input feature-map depth. E.g. (3,3) for a 2D convolution.\n num_filters\
    \ (int): number of filters (output feature-map depth), or ``()`` to denote scalar\
    \ output items (output shape will have no depth axis).\n activation (:class:`~cntk.ops.functions.Function`,\
    \ optional): optional function to apply at the end, e.g. `relu`\n init (scalar\
    \ or NumPy array or :mod:`cntk.initializer`, default :func:`~cntk.initializer.glorot_uniform`):\
    \ initial value of weights `W`\n pad (`bool` or tuple of `bool`\\ s, default `False`):\
    \ if `False`, then the filter will be shifted over the \"valid\"\n  area of input,\
    \ that is, no value outside the area is used. If ``pad=True`` on the other hand,\n\
    \  the filter will be applied to all input positions, and positions outside the\
    \ valid region will be considered containing zero.\n  Use a `tuple` to specify\
    \ a per-axis value.\n strides (`int` or tuple of `int`\\ s, default 1): stride\
    \ of the convolution (increment when sliding the filter over the input). Use a\
    \ `tuple` to specify a per-axis value.\n sharing (`bool`, default `True`): weight\
    \ sharing, must be True for now. \n bias (`bool`, optional, default `True`): the\
    \ layer will have no bias if `False` is passed here\n init_bias (scalar or NumPy\
    \ array or :mod:`cntk.initializer`): initial value of weights `b`\n output_shape\
    \ (`int` or tuple of `int`\\ s): output shape. When strides > 2, the output shape\
    \ is non-deterministic. User can specify the wanted output shape. Note the \n\
    \  specified shape must satisify the condition that if a convolution is perform\
    \ from the output with the same setting, the result must have same shape as the\
    \ input. \n reduction_rank (`int`, default 1): must be 1 for now. \n  that is\
    \ stored with tensor shape (H,W) instead of (1,H,W)\n max_temp_mem_size_in_samples\
    \ (`int`, default 0): set to a positive number to define the maximum workspace\
    \ memory for convolution. \n name (str, optional): the name of the Function instance\
    \ in the network\n\nReturns:\n    :class:`~cntk.ops.functions.Function` that accepts\
    \ one argument and applies the convolution operation to it\n"
  type: Method
  uid: cntk.layers.layers.ConvolutionTranspose
- _type: function
  module: cntk.layers.layers
  name: cntk.layers.layers.ConvolutionTranspose1D
  summary: 'Layer factory function to create a 1D convolution transpose layer with
    optional non-linearity.

    Same as `ConvolutionTranspose()` except that filter_shape is verified to be 1-dimensional.

    See `ConvolutionTranspose()` for extensive documentation.

    '
  type: Method
  uid: cntk.layers.layers.ConvolutionTranspose1D
- _type: function
  module: cntk.layers.layers
  name: cntk.layers.layers.ConvolutionTranspose2D
  summary: 'Layer factory function to create a 2D convolution transpose layer with
    optional non-linearity.

    Same as `ConvolutionTranspose()` except that filter_shape is verified to be 2-dimensional.

    See `ConvolutionTranspose()` for extensive documentation.

    '
  type: Method
  uid: cntk.layers.layers.ConvolutionTranspose2D
- _type: function
  module: cntk.layers.layers
  name: cntk.layers.layers.ConvolutionTranspose3D
  summary: 'Layer factory function to create a 3D convolution transpose layer with
    optional non-linearity.

    Same as `ConvolutionTranspose()` except that filter_shape is verified to be 3-dimensional.

    See `ConvolutionTranspose()` for extensive documentation.

    '
  type: Method
  uid: cntk.layers.layers.ConvolutionTranspose3D
- _type: function
  module: cntk.layers.layers
  name: cntk.layers.layers.Dense
  summary: "Layer factory function to create an instance of a fully-connected linear\
    \ layer of the form\n`activation(input @ W + b)` with weights `W` and bias `b`,\
    \ and `activation` and `b` being optional.\n`shape` may describe a tensor as well.\n\
    \nA ``Dense`` layer instance owns its parameter tensors `W` and `b`, and exposes\
    \ them as attributes ``.W`` and ``.b``.\n\nExample:\n >>> f = Dense(5, activation=C.relu)\n\
    \ >>> x = input(3)\n >>> h = f(x)\n >>> h.shape\n     (5,)\n >>> f.W.shape\n \
    \    (3, 5)\n >>> f.b.value\n     array([ 0.,  0.,  0.,  0.,  0.], dtype=float32)\n\
    \n >>> # activation through default options\n >>> with default_options(activation=C.relu):\n\
    \ ...     f = Dense(500)\n\nArgs:\n shape (`int` or `tuple` of `ints`): vector\
    \ or tensor dimension of the output of this layer\n activation (:class:`~cntk.ops.functions.Function`,\
    \ defaults to identity): optional function to apply at the end, e.g. `relu`\n\
    \ init (scalar or NumPy array or :mod:`cntk.initializer`, defaults to :func:`~cntk.initializer.glorot_uniform`\
    \ ): initial value of weights `W`\n input_rank (int, defaults to `None`): number\
    \ of inferred axes to add to W (`map_rank` must not be given)\n map_rank (int,\
    \ defaults to `None`): expand W to leave exactly `map_rank` axes (`input_rank`\
    \ must not be given)\n bias (bool, optional, defaults to `True`): the layer will\
    \ have no bias if `False` is passed here\n init_bias (scalar or NumPy array or\
    \ :mod:`cntk.initializer`, defualts to 0): initial value of weights `b`\n name\
    \ (str, defaults to ''): the name of the function instance in the network\n\n\
    Returns:\n    cntk.ops.functions.Function: \n    A function that accepts one argument\
    \ and applies the operation to it\n"
  type: Method
  uid: cntk.layers.layers.Dense
- _type: function
  module: cntk.layers.layers
  name: cntk.layers.layers.Dropout
  summary: "Layer factory function to create a drop-out layer.\n\nThe dropout rate\
    \ can be specified as the probability of *dropping* a value (``dropout_rate``).\n\
    E.g. ``Dropout(0.3)`` means \"drop 30% o the activation values.\"\nAlternatively,\
    \ it can also be specified as the probability of *keeping* a value (``keep_prob``).\n\
    \nExample:\n >>> f = Dropout(0.2)   # \"drop 20% of activations\"\n >>> h = input(3)\n\
    \ >>> hd = f(h)\n\n >>> f = Dropout(keep_prob=0.8)   # \"keep 80%\"\n >>> h =\
    \ input(3)\n >>> hd = f(h)\n\nArgs:\n dropout_rate (float): probability of dropping\
    \ out an element, mutually exclusive with ``keep_prob``\n keep_prob (float): probability\
    \ of keeping an element, mutually exclusive with ``dropout_rate``\n name (str,\
    \ defaults to ''): the name of the function instance in the network\n\nReturns:\n\
    \    cntk.ops.functions.Function:\n    A function that accepts one argument and\
    \ applies the operation to it\n"
  type: Method
  uid: cntk.layers.layers.Dropout
- _type: function
  module: cntk.layers.layers
  name: cntk.layers.layers.Embedding
  summary: "Layer factory function to create a embedding layer.\n\nAn embedding is\
    \ conceptually a lookup table. For every input token (e.g. a word or any category\
    \ label), the corresponding\nentry in in the lookup table is returned.\n\nIn CNTK,\
    \ discrete items such as words are represented as one-hot vectors.\nThe table\
    \ lookup is realized as a matrix product, with a matrix\nwhose rows are the embedding\
    \ vectors.\nNote that multiplying a matrix from the left with a one-hot vector\
    \ is the same as copying\nout the row for which the input vector is 1.\nCNTK has\
    \ special optimizations to make this operation as efficient as an actual table\
    \ lookup if the input is sparse.\n\nThe lookup table in this layer is learnable,\n\
    unless a user-specified one is supplied through the ``weights`` parameter.\nFor\
    \ example, to use an existing embedding table from a file in numpy format, use\
    \ this::\n\n  Embedding(weights=np.load('PATH.npy'))\n\nTo initialize a learnable\
    \ lookup table with a given numpy array that is to be used as\nthe initial value,\
    \ pass that array to the ``init`` parameter (not ``weights``).\n\nAn ``Embedding``\
    \ instance owns its weight parameter tensor `E`, and exposes it as an attribute\
    \ ``.E``.\n\nExample:\n >>> # learnable embedding\n >>> f = Embedding(5)\n >>>\
    \ x = input(3)\n >>> e = f(x)\n >>> e.shape\n     (5,)\n >>> f.E.shape\n     (3,\
    \ 5)\n\n >>> # user-supplied embedding\n >>> f = Embedding(weights=[[.5, .3, .1,\
    \ .4, .2], [.7, .6, .3, .2, .9]])\n >>> f.E.value\n     array([[ 0.5,  0.3,  0.1,\
    \  0.4,  0.2],\n            [ 0.7,  0.6,  0.3,  0.2,  0.9]], dtype=float32)\n\
    \ >>> x = input(2, is_sparse=True)\n >>> e = f(x)\n >>> e.shape\n     (5,)\n >>>\
    \ e(C.Value.one_hot([[1], [0], [0], [1]], num_classes=2))\n array([[ 0.7,  0.6,\
    \  0.3,  0.2,  0.9],\n        [ 0.5,  0.3,  0.1,  0.4,  0.2],\n        [ 0.5,\
    \  0.3,  0.1,  0.4,  0.2],\n        [ 0.7,  0.6,  0.3,  0.2,  0.9]], dtype=float32)\n\
    \nArgs:\n shape (`int` or `tuple` of `ints`): vector or tensor dimension of the\
    \ output of this layer\n init (scalar or NumPy array or :mod:`cntk.initializer`,\
    \ defaults to :func:`~cntk.initializer.glorot_uniform` ): (learnable embedding\
    \ only) initial value of weights `E`\n weights (NumPy array, mutually exclusive\
    \ with ``init``, defuats to `None`): (user-supplied embedding only) the lookup\
    \ table.\n  The matrix rows are the embedding vectors, ``weights[i,:]`` being\
    \ the embedding that corresponds to input category `i`.\n name (str, defaults\
    \ to ''): the name of the function instance in the network\n\nReturns:\n    cntk.ops.functions.Function:\n\
    \    A function that accepts one argument and applies the embedding operation\
    \ to it\n"
  type: Method
  uid: cntk.layers.layers.Embedding
- _type: function
  module: cntk.layers.layers
  name: cntk.layers.layers.GlobalAveragePooling
  summary: "Layer factory function to create a global average-pooling layer.\n\nThe\
    \ global average-pooling operation computes the element-wise mean over all items\
    \ on an N-dimensional grid, such as an image.\n\nThis operation is the same as\
    \ applying ``reduce_mean()`` to all grid dimensions.\n\nExample:\n >>> f = GlobalAveragePooling()\n\
    \ >>> f.update_signature((1,4,4))\n >>> im = np.array([[[3, 5, 2, 6], [4, 2, 8,\
    \ 3], [1, 6, 4, 7], [7, 3, 5, 9]]])  # a 4x4 image (feature-map depth 1 for simplicity)\n\
    \ >>> im\n     array([[[3, 5, 2, 6],\n             [4, 2, 8, 3],\n           \
    \  [1, 6, 4, 7],\n             [7, 3, 5, 9]]])\n >>> f([[im]])\n     array([[[[\
    \ 4.6875]]]], dtype=float32)\n\nArgs:\n name (str, defaults to ''): the name of\
    \ the function instance in the network\n\nReturns:\n    cntk.ops.functions.Function:\n\
    \    A function that accepts one argument and applies the operation to it\n"
  type: Method
  uid: cntk.layers.layers.GlobalAveragePooling
- _type: function
  module: cntk.layers.layers
  name: cntk.layers.layers.GlobalMaxPooling
  summary: "Layer factory function to create a global max-pooling layer.\n\nThe global\
    \ max-pooling operation computes the element-wise maximum over all items on an\
    \ N-dimensional grid, such as an image.\n\nThis operation is the same as applying\
    \ ``reduce_max()`` to all grid dimensions.\n\nExample:\n >>> f = GlobalMaxPooling()\n\
    \ >>> f.update_signature((1,4,4))\n >>> im = np.array([[[3, 5, 2, 6], [4, 2, 8,\
    \ 3], [1, 6, 4, 7], [7, 3, 5, 9]]])  # a 4x4 image (feature-map depth 1 for simplicity)\n\
    \ >>> im\n     array([[[3, 5, 2, 6],\n             [4, 2, 8, 3],\n           \
    \  [1, 6, 4, 7],\n             [7, 3, 5, 9]]])\n >>> f([[im]])\n     array([[[[\
    \ 9.]]]], dtype=float32)\n\nArgs:\n name (str, defaults to ''): the name of the\
    \ function instance in the network\n\nReturns:\n    cntk.ops.functions.Function:\n\
    \    A function that accepts one argument and applies the operation to it\n"
  type: Method
  uid: cntk.layers.layers.GlobalMaxPooling
- _type: function
  module: cntk.layers.layers
  name: cntk.layers.layers.Label
  summary: "Layer factory function to create a dummy layer with a given name.\nThis\
    \ can be used to access an intermediate value flowing through computation.\n\n\
    Args:\n name (str): the name of the function instance in the network\n\nExample:\n\
    \ >>> model = Dense(500) >> Label('hidden') >> Dense(10)\n >>> model.update_signature(10)\n\
    \ >>> intermediate_val = model.hidden\n >>> intermediate_val.shape\n     (500,)\n\
    \nReturns:\n    cntk.ops.functions.Function:\n    A function that accepts one\
    \ argument and returns it with the desired name attached\n"
  type: Method
  uid: cntk.layers.layers.Label
- _type: function
  module: cntk.layers.layers
  name: cntk.layers.layers.LayerNormalization
  summary: "Layer factory function to create a function that implements layer normalization.\n\
    \nLayer normalization applies this formula to every input element (element-wise):\n\
    ``y = (x - mean(x)) / (stddev(x) + epsilon) * scale + bias``\nwhere ``scale``\
    \ and ``bias`` are learned scalar parameters.\nTODO: add paper reference\n\nExample:\n\
    \ >>> f = LayerNormalization(initial_scale=2, initial_bias=1)\n >>> f.update_signature(4)\n\
    \ >>> f([np.array([4,0,0,4])])  # result has mean 1 and standard deviation 2,\
    \ reflecting the initial values for scale and bias\n     array([[ 2.99999, -0.99999,\
    \ -0.99999,  2.99999]], dtype=float32)\n\nArgs:\n initial_scale (float, default\
    \ 1): initial value for the ``scale`` parameter\n initial_bias (float, default\
    \ 0): initial value for the ``bias`` parameter\n epsilon (float, default 0.00001):\
    \ epsilon added to the standard deviation to avoid division by 0\n name (str,\
    \ optional): the name of the Function instance in the network\n\nReturns:\n  \
    \  cntk.ops.functions.Function:\n    A function that accepts one argument and\
    \ applies the operation to it\n"
  type: Method
  uid: cntk.layers.layers.LayerNormalization
- _type: function
  module: cntk.layers.layers
  name: cntk.layers.layers.MaxPooling
  summary: "Layer factory function to create a max-pooling layer.\n\nLike ``Convolution()``,\
    \ ``MaxPooling()`` processes items arranged on an N-dimensional grid, such as\
    \ an image.\nTypically, each item is a vector.\nFor each item, max-pooling computes\
    \ the element-wise maximum over a window (\"receptive field\") of items surrounding\
    \ the item's position on the grid.\n\nThe size (spatial extent) of the receptive\
    \ field is given by ``filter_shape``.\nE.g. for 2D pooling, ``filter_shape`` should\
    \ be a tuple of two integers, such as `(5,5)`.\n\nExample:\n >>> f = MaxPooling((3,3),\
    \ strides=2)  # reduce dimensionality by 2, pooling over windows of 3x3\n >>>\
    \ h = input((32,240,320))  # e.g. 32-dim feature map\n >>> hp = f(h)\n >>> hp.shape\
    \  # spatial dimension has been halved due to stride, and lost one due to 3x3\
    \ window without padding\n     (32, 119, 159)\n\n >>> f = MaxPooling((2,2), strides=2)\n\
    \ >>> f.update_signature((1,4,4))\n >>> im = np.array([[[3, 5, 2, 6], [4, 2, 8,\
    \ 3], [1, 6, 4, 7], [7, 3, 5, 9]]])  # a 4x4 image (feature-map depth 1 for simplicity)\n\
    \ >>> im\n     array([[[3, 5, 2, 6],\n             [4, 2, 8, 3],\n           \
    \  [1, 6, 4, 7],\n             [7, 3, 5, 9]]])\n >>> f([[im]])  # due to strides=2,\
    \ this picks the max out of each 2x2 sub-block\n     array([[[[ 5.,  8.],\n  \
    \            [ 7.,  9.]]]], dtype=float32)\n\nArgs:\n filter_shape (`int` or `tuple`\
    \ of `ints`): shape (spatial extent) of the receptive field, *not* including the\
    \ input feature-map depth. E.g. (3,3) for a 2D convolution.\n strides (`int` or\
    \ `tuple` of `ints`, defaults to 1): stride (increment when sliding over the input).\
    \ Use a `tuple` to specify a per-axis value.\n pad (`bool` or `tuple` of `bools`,\
    \ defaults to `False`): if `False`, then the pooling operation will be shifted\
    \ over the \"valid\"\n  area of input, that is, no value outside the area is used.\
    \ If ``pad=True`` on the other hand,\n  pooling will be applied to all input positions,\
    \ and positions outside the valid region will be considered containing zero.\n\
    \  Use a `tuple` to specify a per-axis value.\n name (str, defaults to ''): the\
    \ name of the function instance in the network\n\nReturns:\n    cntk.ops.functions.Function:\n\
    \    A function that accepts one argument and applies the max-pooling operation\
    \ to it\n"
  type: Method
  uid: cntk.layers.layers.MaxPooling
- _type: function
  module: cntk.layers.layers
  name: cntk.layers.layers.MaxUnpooling
  summary: ''
  type: Method
  uid: cntk.layers.layers.MaxUnpooling
