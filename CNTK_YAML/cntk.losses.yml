api_name: []
items:
- _type: module
  children: []
  module: cntk.losses
  name: cntk.losses
  summary: ''
  type: Namespace
  uid: cntk.losses
- _type: class
  children:
  - cntk.losses.binary_cross_entropy
  - cntk.losses.cosine_distance
  - cntk.losses.cosine_distance_with_negative_samples
  - cntk.losses.cross_entropy_with_softmax
  - cntk.losses.lambda_rank
  - cntk.losses.squared_error
  - cntk.losses.weighted_binary_cross_entropy
  module: cntk.losses
  name: cntk.losses.Global
  summary: Proxy object to hold module level functions
  type: Class
  uid: cntk.losses.Global
- _type: function
  module: cntk.losses
  name: cntk.losses.binary_cross_entropy
  summary: "Computes the binary cross entropy (aka logistic loss) between the ``output``\
    \ and ``target``.\n\nExample:\n    TBA\n\nArgs:\n    output: the computed posterior\
    \ probability for a variable to be 1 from the network (typ. a ``sigmoid``)\n \
    \   target: ground-truth label, 0 or 1\n    name (str, optional): the name of\
    \ the Function instance in the network\nReturns:\n    :class:`~cntk.ops.functions.Function`\n"
  type: Method
  uid: cntk.losses.binary_cross_entropy
- _type: function
  module: cntk.losses
  name: cntk.losses.cosine_distance
  summary: "Computes the cosine distance between ``x`` and ``y``:\n\nExample:\n  \
    \  >>> a = np.asarray([-1, -1, -1, 1, 1, -1, 1, 1, -1, 1, 1, -1]).reshape(3,2,2)\n\
    \    >>> b = np.asarray([1, 1, -1, 1, 1, -1, 1, -1, -1, -1, -1, 1]).reshape(3,2,2)\n\
    \    >>> x = C.sequence.input(shape=(2,))\n    >>> y = C.sequence.input(shape=(2,))\n\
    \    >>> np.round(C.cosine_distance(x,y).eval({x:a,y:b}),5)\n    array([[-1.,\
    \  1.],\n           [ 1.,  0.],\n           [ 0., -1.]], dtype=float32)\n\nArgs:\n\
    \    x: numpy array or any :class:`~cntk.ops.functions.Function` that outputs\
    \ a tensor\n    name (str, optional): the name of the Function instance in the\
    \ network\nReturns:\n    :class:`~cntk.ops.functions.Function`\n"
  type: Method
  uid: cntk.losses.cosine_distance
- _type: function
  module: cntk.losses
  name: cntk.losses.cosine_distance_with_negative_samples
  summary: "Given minibatches for ``x`` and ``y``, this function computes for each\
    \ element in `x` the cosine distance between \nit and the corresponding `y` and\
    \ additionally the cosine distance between ``x`` and some other elements of ``y``\
    \ \n(referred to a negative samples). The ``x`` and ``y`` pairs are samples often\
    \ derived \nfrom embeddings of textual data, though the function can be used for\
    \ any form of numeric encodings. \nWhen using this function to compute textual\
    \ similarity, ``x`` represents search query term embedding \nand ``y`` represents\
    \ a document embedding. The negative samples are formed on the fly by shifting\
    \ \nthe right side (``y``). The ``shift`` indicates how many samples in ``y``\
    \ one should shift while\nforming each negative sample pair. It is often chosen\
    \ to be 1. As the name suggests \n``num_negative_samples`` indicates how many\
    \ negative samples one would want to generate.\n\nExample:\n    >>> qry = np.asarray([1.,\
    \ 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1.], dtype=np.float32).reshape(3, 1,\
    \ 4)\n    >>> doc = np.asarray([1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1.],\
    \ dtype=np.float32).reshape(3, 1, 4)\n    >>> x = C.sequence.input(shape=(4,))\n\
    \    >>> y = C.sequence.input(shape=(4,))\n    >>> model = C.cosine_distance_with_negative_samples(x,\
    \ y, shift=1, num_negative_samples=2)\n    >>> np.round(model.eval({x: qry, y:\
    \ doc}), decimals=4)\n    array([[[ 1. ,  0.5,  0. ]],\n    <BLANKLINE>\n    \
    \       [[ 1. ,  0.5,  0.5]],\n    <BLANKLINE>\n           [[ 1. ,  0. ,  0.5]]],\
    \ dtype=float32)\n\nArgs:\n    x: numpy array or any :class:`~cntk.ops.functions.Function`\
    \ that outputs a tensor\n    y: numpy array or any :class:`~cntk.ops.functions.Function`\
    \ that outputs a tensor\n    shift: non-zero positive integer representing number\
    \ of shift to generate a negative sample\n    num_negative_samples: number of\
    \ negative samples to generate, a non-zero positive integer \n    name (str, optional):\
    \ the name of the Function instance in the network\nReturns:\n    :class:`~cntk.ops.functions.Function`\n"
  type: Method
  uid: cntk.losses.cosine_distance_with_negative_samples
- _type: function
  module: cntk.losses
  name: cntk.losses.cross_entropy_with_softmax
  summary: "This operation computes the cross entropy between the ``target_vector``\
    \ and\nthe softmax of the ``output_vector``. The elements of ``target_vector``\n\
    have to be non-negative and should sum to 1. The ``output_vector`` can\ncontain\
    \ any values. The function will internally compute the softmax of\nthe ``output_vector``.\
    \ Concretely,\n\n:math:`\\mathrm{softmax}(x)=\\left[\\frac{\\exp(x_1)}{\\sum_i\\\
    exp(x_i)}\\quad\\frac{\\exp(x_1)}{\\sum_i\\exp(x_i)}\\quad\\ldots\\quad\\frac{\\\
    exp(x_1)}{\\sum_i\\exp(x_i)}\\right]`\n\n:math:`\\mathrm{cross\\_entropy\\_with\\\
    _softmax}(o, t) = -\\sum_{i} t_i \\log(\\mathrm{softmax}(o)_i)`\n\nwith the understanding\
    \ that the implementation can use equivalent formulas\nfor efficiency and numerical\
    \ stability.\n\nExample:\n    >>> C.cross_entropy_with_softmax([[1., 1., 1., 50.]],\
    \ [[0., 0., 0., 1.]]).eval()\n    array([[ 0.]], dtype=float32)\n\n    >>> C.cross_entropy_with_softmax([[1.,\
    \ 2., 3., 4.]], [[0.35, 0.15, 0.05, 0.45]]).eval()\n    array([[ 1.84019]], dtype=float32)\n\
    \nArgs:\n    output_vector: the unscaled computed output values from the network\n\
    \    target_vector: usually it is one-hot vector where the hot bit\n     corresponds\
    \ to the label index. But it can be any probability\n     distribution over the\
    \ labels.\n    axis (int or :class:`~cntk.axis.Axis`, optional): if given, cross\
    \ entropy will be computed\n            along this axis\n    name (str, optional):\
    \ the name of the Function instance in the network\nReturns:\n    :class:`~cntk.ops.functions.Function`\n"
  type: Method
  uid: cntk.losses.cross_entropy_with_softmax
- _type: function
  module: cntk.losses
  name: cntk.losses.lambda_rank
  summary: "Groups samples according to ``group``, sorts\nthem within each group based\
    \ on ``output`` and\ncomputes the Normalized Discounted Cumulative Gain\n(NDCG)\
    \ at infinity for each group. Concretely,\nthe Discounted Cumulative Gain (DCG)\
    \ at infinity is:\n\n:math:`\\mathrm{DCG_{\\infty}}()=\\sum_{i=0}^{\\infty} \\\
    frac{gain_{(i)}}{\\log(i+2)}`\n\nwhere :math:`gain_{(i)}` means the gain of the\
    \ :math:`i`-th ranked sample.\n\nThe NDCG is just the DCG  divided by the maximum\
    \ achievable DCG (obtained\nby placing the samples with the largest gain at the\
    \ top of the ranking).\n\nSamples in the same group must appear in order of decreasing\
    \ gain.\n\nIt returns 1 minus the average NDCG across all the groups in the minibatch\n\
    multiplied by 100 times the number of samples in the minibatch.\n\nIn the backward\
    \ direction it back-propagates LambdaRank gradients.\n\nExample:\n    >>> group\
    \ = C.input((1,))\n    >>> score = C.input((1,), needs_gradient=True)\n    >>>\
    \ gain  = C.input((1,))\n    >>> g = np.array([1, 1, 2, 2], dtype=np.float32).reshape(4,1)\n\
    \    >>> s = np.array([1, 2, 3, 4], dtype=np.float32).reshape(4,1)\n    >>> n\
    \ = np.array([7, 1, 3, 1], dtype=np.float32).reshape(4,1)\n    >>> f = C.lambda_rank(score,\
    \ gain, group)\n    >>> np.round(f.grad({score:s, gain:n, group: g}, wrt=[score]),4)\n\
    \    array([[-0.2121],\n    <BLANKLINE>\n           [ 0.2121],\n    <BLANKLINE>\n\
    \           [-0.1486],\n    <BLANKLINE>\n           [ 0.1486]], dtype=float32)\n\
    \nArgs:\n    output: score of each sample\n    gain: gain of each sample\n   \
    \ group: group of each sample\n    name (str, optional): the name of the Function\
    \ instance in the network\nReturns:\n    :class:`~cntk.ops.functions.Function`\n"
  type: Method
  uid: cntk.losses.lambda_rank
- _type: function
  module: cntk.losses
  name: cntk.losses.squared_error
  summary: "This operation computes the sum of the squared difference between elements\n\
    in the two input matrices. The result is a scalar (i.e., one by one matrix).\n\
    This is often used as a training criterion.\n\nExample:\n    >>> i1 = C.input((1,2))\n\
    \    >>> i2 = C.input((1,2))\n    >>> C.squared_error(i1,i2).eval({i1:np.asarray([[[2.,\
    \ 1.]]], dtype=np.float32), i2:np.asarray([[[4., 6.]]], dtype=np.float32)})\n\
    \    array([ 29.], dtype=float32)\n\n    >>> C.squared_error(i1,i2).eval({i1:np.asarray([[[1.,\
    \ 2.]]], dtype=np.float32), i2:np.asarray([[[1., 2.]]], dtype=np.float32)})\n\
    \    array([ 0.], dtype=float32)\n\nArgs:\n    output: the output values from\
    \ the network\n    target: it is usually a one-hot vector where the hot bit\n\
    \     corresponds to the label index\n    name (str, optional): the name of the\
    \ Function instance in the network\nReturns:\n    :class:`~cntk.ops.functions.Function`\n"
  type: Method
  uid: cntk.losses.squared_error
- _type: function
  module: cntk.losses
  name: cntk.losses.weighted_binary_cross_entropy
  summary: "This operation computes the weighted binary cross entropy (aka logistic\
    \ loss) between the ``output`` and ``target``.\n\nExample:\n    TBA\n\nArgs:\n\
    \    output: the computed posterior probability from the network\n    target:\
    \ ground-truth label, 0 or 1\n    weight: weight of each example\n    name (str,\
    \ optional): the name of the Function instance in the network\nReturns:\n    :class:`~cntk.ops.functions.Function`\n"
  type: Method
  uid: cntk.losses.weighted_binary_cross_entropy
