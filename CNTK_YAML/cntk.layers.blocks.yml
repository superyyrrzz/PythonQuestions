api_name: []
items:
- _type: module
  children: []
  module: cntk.layers.blocks
  name: cntk.layers.blocks
  summary: "blocks -- basic building blocks that are semantically not layers (not\
    \ used in a layered fashion)\n          e.g. the LSTM\n"
  type: Namespace
  uid: cntk.layers.blocks
- _type: class
  children:
  - cntk.layers.blocks.BlockFunction
  - cntk.layers.blocks.ForwardDeclaration
  - cntk.layers.blocks.GRU
  - cntk.layers.blocks.LSTM
  - cntk.layers.blocks.RNNUnit
  - cntk.layers.blocks.Stabilizer
  - cntk.layers.blocks.UntestedBranchError
  module: cntk.layers.blocks
  name: cntk.layers.blocks.Global
  summary: Proxy object to hold module level functions
  type: Class
  uid: cntk.layers.blocks.Global
- _type: function
  module: cntk.layers.blocks
  name: cntk.layers.blocks.BlockFunction
  summary: 'Decorator for defining a @Function as a BlockFunction. Same as @Function,
    but wrap the content into an as_block().

    '
  type: Method
  uid: cntk.layers.blocks.BlockFunction
- _type: function
  module: cntk.layers.blocks
  name: cntk.layers.blocks.ForwardDeclaration
  summary: "Helper for recurrent network declarations.\nReturns a placeholder variable\
    \ with an added method resolve_to() to be called\nat the end to close the loop.\n\
    This is used for explicit graph building with recurrent connections.\n\nExample:\n\
    \ >>> # create a graph with a recurrent loop to compute the length of an input\
    \ sequence\n >>> from cntk.layers.typing import *\n >>> x = C.input(**Sequence[Tensor[2]])\n\
    \ >>> ones_like_input = sequence.broadcast_as(1, x)  # sequence of scalar ones\
    \ of same length as input\n >>> out_fwd = ForwardDeclaration()  # placeholder\
    \ for the state variables\n >>> out = past_value(out_fwd, initial_state=0) + ones_like_input\n\
    \ >>> out_fwd.resolve_to(out)\n >>> length = sequence.last(out)\n >>> x0 = np.reshape(np.arange(6,dtype=np.float32),(1,3,2))\n\
    \ >>> x0\n     array([[[ 0.,  1.],\n             [ 2.,  3.],\n             [ 4.,\
    \  5.]]], dtype=float32)\n >>> length(x0)\n     array([[ 3.]], dtype=float32)\n\
    \nReturns:\n    a placeholder variable with a method ``resolve_to()`` that resolves\
    \ it to another variable\n"
  type: Method
  uid: cntk.layers.blocks.ForwardDeclaration
- _type: function
  module: cntk.layers.blocks
  name: cntk.layers.blocks.GRU
  summary: "Layer factory function to create a GRU block for use inside a recurrence.\n\
    The GRU block implements one step of the recurrence and is stateless. It accepts\
    \ the previous state as its first argument,\nand outputs its new state.\n\nExample:\n\
    \ >>> # a gated recurrent layer\n >>> from cntk.layers import *\n >>> gru_layer\
    \ = Recurrence(GRU(500))\n\nArgs:\n    shape (`int` or `tuple` of `ints`): vector\
    \ or tensor dimension of the output of this layer\n    cell_shape (tuple, defaults\
    \ to `None`): if given, then the output state is first computed at `cell_shape`\n\
    \     and linearly projected to `shape`\n    activation (:class:`~cntk.ops.functions.Function`,\
    \ defaults to :func:`~cntk.ops.tanh`): function to apply at the end, e.g. `relu`\n\
    \    init (scalar or NumPy array or :mod:`cntk.initializer`, defaults to `glorot_uniform`):\
    \ initial value of weights `W`\n    init_bias (scalar or NumPy array or :mod:`cntk.initializer`,\
    \ defaults to 0): initial value of weights `b`\n    enable_self_stabilization\
    \ (bool, defaults to `False`): if `True` then add a :func:`~cntk.layers.blocks.Stabilizer`\n\
    \     to all state-related projections (but not the data input)\n    name (str,\
    \ defaults to ''): the name of the Function instance in the network\n\nReturns:\n\
    \    cntk.ops.functions.Function:\n    A function ``(prev_h, input) -> h`` that\
    \ implements one step of a recurrent GRU layer.\n"
  type: Method
  uid: cntk.layers.blocks.GRU
- _type: function
  module: cntk.layers.blocks
  name: cntk.layers.blocks.LSTM
  summary: "Layer factory function to create an LSTM block for use inside a recurrence.\n\
    The LSTM block implements one step of the recurrence and is stateless. It accepts\
    \ the previous state as its first two arguments,\nand outputs its new state as\
    \ a two-valued tuple ``(h,c)``.\n\nExample:\n >>> # a typical recurrent LSTM layer\n\
    \ >>> from cntk.layers import *\n >>> lstm_layer = Recurrence(LSTM(500))\n\nArgs:\n\
    \    shape (`int` or `tuple` of `ints`): vector or tensor dimension of the output\
    \ of this layer\n    cell_shape (tuple, defaults to `None`): if given, then the\
    \ output state is first computed at `cell_shape`\n     and linearly projected\
    \ to `shape`\n    activation (:class:`~cntk.ops.functions.Function`, defaults\
    \ to :func:`~cntk.ops.tanh`): function to apply at the end, e.g. `relu`\n    use_peepholes\
    \ (bool, defaults to `False`):\n    init (scalar or NumPy array or :mod:`cntk.initializer`,\
    \ defaults to `glorot_uniform`): initial value of weights `W`\n    init_bias (scalar\
    \ or NumPy array or :mod:`cntk.initializer`, defaults to 0): initial value of\
    \ weights `b`\n    enable_self_stabilization (bool, defaults to `False`): if `True`\
    \ then add a :func:`~cntk.layers.blocks.Stabilizer`\n     to all state-related\
    \ projections (but not the data input)\n    name (str, defaults to ''): the name\
    \ of the Function instance in the network\n\nReturns:\n    cntk.ops.functions.Function:\n\
    \    A function ``(prev_h, prev_c, input) -> (h, c)`` that implements one step\
    \ of a recurrent LSTM layer.\n"
  type: Method
  uid: cntk.layers.blocks.LSTM
- _type: function
  module: cntk.layers.blocks
  name: cntk.layers.blocks.RNNUnit
  summary: "Layer factory function to create a plain RNN block for use inside a recurrence.\n\
    The RNN block implements one step of the recurrence and is stateless. It accepts\
    \ the previous state as its first argument,\nand outputs its new state.\n\nExample:\n\
    \ >>> # a plain relu RNN layer\n >>> from cntk.layers import *\n >>> relu_rnn_layer\
    \ = Recurrence(RNNUnit(500, activation=C.relu))\n\nArgs:\n    shape (`int` or\
    \ `tuple` of `ints`): vector or tensor dimension of the output of this layer\n\
    \    cell_shape (tuple, defaults to `None`): if given, then the output state is\
    \ first computed at `cell_shape`\n     and linearly projected to `shape`\n   \
    \ activation (:class:`~cntk.ops.functions.Function`, defaults to signmoid): function\
    \ to apply at the end, e.g. `relu`\n    init (scalar or NumPy array or :mod:`cntk.initializer`,\
    \ defaults to `glorot_uniform`): initial value of weights `W`\n    init_bias (scalar\
    \ or NumPy array or :mod:`cntk.initializer`, defaults to 0): initial value of\
    \ weights `b`\n    enable_self_stabilization (bool, defaults to `False`): if `True`\
    \ then add a :func:`~cntk.layers.blocks.Stabilizer`\n     to all state-related\
    \ projections (but not the data input)\n    name (str, defaults to ''): the name\
    \ of the Function instance in the network\n\nReturns:\n    cntk.ops.functions.Function:\n\
    \    A function ``(prev_h, input) -> h`` where ``h = activation(input @ W + prev_h\
    \ @ R + b)``\n"
  type: Method
  uid: cntk.layers.blocks.RNNUnit
- _type: function
  module: cntk.layers.blocks
  name: cntk.layers.blocks.Stabilizer
  summary: "Layer factory function to create a `Droppo self-stabilizer <https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/SelfLR.pdf>`_.\n\
    It multiplies its input with a scalar that is learned.\n\nThis takes `enable_self_stabilization`\
    \ as a flag that allows to disable itself. Useful if this is a global default.\n\
    \nNote: Some other layers (specifically, recurrent units like :func:`~cntk.layers.blocks.LSTM`)\
    \ also have the option to\nuse the ``Stabilizer()`` layer internally. That is\
    \ enabled by passing `enable_self_stabilization=True`\nto those layers. In conjunction\
    \ with those, the rule is that an explicit ``Stabilizer()`` must be\ninserted\
    \ by the user for the main data input, whereas the recurrent layer will own the\
    \ stabilizer(s)\nfor the internal recurrent connection(s).\nNote: Unlike the original\
    \ paper, which proposed a linear or exponential scalar,\nCNTK uses a sharpened\
    \ Softplus: 1/steepness ln(1+e^{steepness*beta}).\nThe softplus behaves linear\
    \ for weights around and above 1 (like the linear scalar) while guaranteeing\n\
    positiveness (like the exponentional variant) but is also more robust by avoiding\
    \ exploding gradients.\n\nExample:\n >>> # recurrent model with self-stabilization\n\
    \ >>> from cntk.layers import *\n >>> with default_options(enable_self_stabilization=True):\
    \ # enable stabilizers by default for LSTM()\n ...     model = Sequential([\n\
    \ ...         Embedding(300),\n ...         Stabilizer(),           # stabilizer\
    \ for main data input of recurrence\n ...         Recurrence(LSTM(512)),  # LSTM\
    \ owns its own stabilizers for the recurrent connections\n ...         Stabilizer(),\n\
    \ ...         Dense(10)\n ...     ])\n\nArgs:\n    steepness (`int`, defaults\
    \ to 4):\n    enable_self_stabilization (bool, defaults to `False`): a flag that\
    \ allows to disable itself. Useful if this is a global default\n    name (str,\
    \ defaults to ''): the name of the Function instance in the network\n\nReturns:\n\
    \    cntk.ops.functions.Function:\n    A function\n"
  type: Method
  uid: cntk.layers.blocks.Stabilizer
- _type: function
  module: cntk.layers.blocks
  name: cntk.layers.blocks.UntestedBranchError
  summary: ''
  type: Method
  uid: cntk.layers.blocks.UntestedBranchError
